[[journal-design.pdf]]
# Journaling the Linux ext2fs Filesystem
## Abstract
本文介绍 Linux ext2fs 文件系统的事务型日志设计，用于崩溃后的文件系统恢复，该功能提升了系统的性能与可靠性

## Introduction
**研究背景**
硬盘容量不断增大，而崩溃恢复算法却没有改进，导致时间复杂度成线性增长，故需要一种更好的恢复设计

### What's in a filesystem?
OS 对文件系统的接口形式有明显要求，但是对其内部实现却是放任自由的，这就导致不同的文件系统实现呈现出不同特点 

不过实现方式再怎么自由，还是有三条准则：
1. 性能不要太差
2. 必须与旧应用兼容
3. 必须保证文件系统的可靠性

### Filesystem Reliability
确保文件系统的稳定性，考虑以下三点：
1. 保护 crash 之前已经存在的数据
2. crash 之前的系统状态必须是可预测的
3. 一个文件事务必须保证其原子性

### Existing implementations
ext2fs 具备保护功能，但是不提供可预测性与原子性

拥有 crash 前的*写操作序列*，才能得知应该恢复到何种状态

要对磁盘写操作进行排序，最简单的方法就是*舍弃并发性*，让写操作严格串行执行

然而这种方法简单粗暴，在 IO 等待期间浪费了大量时间，也不能将写操作集中到同一个 block 再*延迟*提交，对性能的危害是显然的

一种解决方法是，为写操作标记*时间戳*，记录 buffer 的 IO 序列，但是在某个操作的 predecessors 全部完成前，该操作不能写回 disk

但是这种方法导致*循环等待*的问题，因为一个事务可能写入多个块，与其他事务构成死锁

解决死锁的方案是，检测到循环等待，就不断 *rollback* 环中的一个事务，直到系统得以继续运行

然而这些方法都有一个共性问题，recover 进程仍然需要扫描整个 disk，可靠性是有了，但是性能还是一团糟 

要解决这个问题，需要保证事务的原子性，在 *commit* 日志写入磁盘后，该事务才视为已完成。如果一个事务没有 commit 就 crush 了，认为该事务不可靠，必须 rollback，这就需要文件系统既要记录一个 uptade 的新值，也要记录 update 的旧值

有很多种方法实现原子性，可以在 copy 新值时创建一个*副本*，等到 commit 完成后再*回收*旧值的空间，*WAFL文件系统* 采用这种实现

同样，log结构系统通过 log *连续流*写入数据（包括metadata），实现了同样的功能，而且可以提供 *checkpoint* 功能，还具有较少的磁盘 IO 次数，性能不错

此外，还有一种自动提交的文件系统，它们将新的更新保存为副本，不过在 commit 之后会写回到原来的位置，而不是回收旧空间

下面介绍 **Journaling** 系统的工作方式：
- 有关 metadata 的更新会写入到磁盘的指定位置
- 文件内容写入成功后，会提交 commit 到磁盘，之后再更新 metadata
- 如果恢复时没有 commit ，就 undo 事务
- 如果有 commit，就 redo 事务
- 不管有没有 commit，事务都得到了处理，保证原子性

## Designing a new filesystem for Linux
本系统的设计目的是减少 recovery 时间，因此我们选择 journaling 模式作为基础，该模式可以通过扫描日志来恢复系统，而日志占用的空间极少，并且覆盖了所有可能出差错的数据，**性能和可靠性**都很好

另一个优点是，该系统将日志文件保存在一个*新的逻辑段*，该段专门用于储存日志，因此对现有的 file 和 metadata 的存储方式不做要求，提供了对旧系统的**兼容性**

所以我们不会从头设计一个新系统，而是在现有的 ext2fs 上添加新特性--**事务 journal**

### Anatomy of a transaction
作为该系统的一个核心概念，事务不仅会修改文件内容，也会修改 metadata（timestamp、位图等）

同时事务还需要读文件，这就对调度提出了顺序要求

如果一个事务删除了 something 文件，而另一个事务又添加了 something 同名文件，显然二者在逻辑上具有偏序关系

因此在 commit 事务前，必须确保所有有关的 block 已经被写回磁盘，以保证 fs 的可靠性

### Merging transactions
事务的相关术语大多来自 database 领域，然而传统的数据库事务与文件系统并不完全相同，在 fs 中，事情会更简单些

有两个区别：
1. 文件系统中没有 `abort`
2. 文件系统的事务持续时间相对较短

一般在写入开始前，fs 就会检查事务的合法性，如果不合法，由于还没进行任何写操作，直接提交也无妨，没必要 abort

而第二个区别更加关键，由于短事务的特性，我们可以直接按照严格串行的顺序执行，而不需要进行并发调度，这一点又提高了性能

通过以上观察，我们可以简化事务模型：与其进行大量小而频繁的操作，不如将它们定期集成到一个大事务里，集中 commit

该设计的优点是：减少了修改 metadata 的频率，进一步提高性能

关于集中事务的定期时长，这是一个策略（policy）问题，由用户决定，越长越省时间，但恢复成本也更高

### On-disk representation
新系统与原 ext2fs 的物理存储方式兼容，通过将 journal 块绑定到现有的 inode 实现兼容性

同时，为 journal 设定一个新的扩展位，这样即使一个老内核挂载了一个新ext2fs，也不能对新系统做任何修改

### Format of the filesystem journal
日志文件的工作：记录 metadata 的变化，保证事务的原子性

我们在 journal 中写入三种 block：metadata、descriptor、header

metadata 日志块维护 metadata 的更新历史，这意味着即使有一点点更改，也要重新写入整个日志块，不过，可以证明这样做的成本并不高：
- 日志读写的速度很快，也可以集中提交
- 由于时间局部性，要写的数据通常就在 cache 中，节省了 CPU IO 时间

Linux 对缓存区写回的优化很好，如果不想影响现有的 *buffer_head*，就 copy 一个临时 buffer_head，写入后再删除该临时副本

descriptor 日志块描述其他 metadata 日志块对应的实际物理位置，从而将修改应用到正确的地方，在所有 metadata 日志块之前写回，并包括 metadata 应写回的区间

日志块按循环队列的形式写入，每当资源耗尽就清理空间

header 日志块被固定到一个位置，存储 *日志的metadata* ，标记头尾位置

recover 时先扫描 header 确定日志位置，再检查其他日志块进行恢复

> *为什么只记录 metadata 而不记录用户数据?*
> 对全部数据都写入 journal 的开销太大了，因此将用户数据的保护交给其他应用层的程序，也可以配置 journal 模式使其记录全部日志
> 文件系统对文件数据的要求，并没有数据库那么严格

### Committing and checkpointing the journal
有时事务积攒的过多，我们希望能够强制提交当前的事务，并释放一部分空间，然而 commit 并不代表工作结束了，在此之后还要修改 metadata，而 metadata 的写入操作还被保存在 journal block 中

完全实现检查点功能，分为以下阶段：
1. 封闭当前事务，未尽的文件操作可以继续在旧事务中执行，而新开始的需要加入到新事务中
2. 开始将旧事务的修改 *buffer* 写回到磁盘（flush），写回完成前将该 buffer 钉死（pin）
3. *等待*旧事务的所有文件操作完成，在此期间允许并行写入新 log
4. 等待所有事务 update 操作被写入 journal
5. 更新 journal header block，更新日志的头尾指针
6. 当所有 metadata buffer 被写回到磁盘后，标志着事务安全完成，此时可以*重用*旧事务占用的 journal 块

### Collisions between 
为提高性能，我们不得不允许事务间的并发执行，而这难免会导致冲突，前文提到过，OS 的文件事务较为短小，因此可以视为严格串行，所以不会发生数据不一致的问题

然而，这仅仅是 buffer 区内的保证，对于写回到磁盘的数据，如果一个新事务修改了还没来得及 commit 的 buffer，就会导致提交错误的数据，造成逻辑错误，但是又不能不让新事务工作

这种情况的解决方案是：为新事物 copy 一个 buffer 副本，专门用于更新，而旧的 buffer 在 commit 完成后就删除，这样新旧事务就不会相互制约了

## Project status and future work
该系统仍需要改进，目前设计兼顾了简洁性与稳定性，我们暂时还不需要开发新的 major 版本

上面的设计只需要在 ext2fs 的基础上增加代码即可，比较好实现

一旦我们实现了该系统的稳定版本，我们会考虑以下的研究方向：研究参数对性能的影响、现有设计的性能瓶颈，扩展现有的系统功能

我们的一个研究主题可能会是 *改进日志的写入方式* ，当前系统将整个 buffer 写入日志区，我们觉得也可以不记录数据内容，而是记录*数据变化*，只记录新值和旧值，尚未确定这个会对性能有多大的优化，因为目前的实现没有 memory-to-memory 的数据传递，大大提升了 IO 效率，性能已经足够

另一个研究主题是针对 *NFS* 服务器的功能扩展，在客户端和服务端之间也实现这样的 journal 恢复协议

不过这个方向可能不太适用，因为 NFS 的性能评价指标是 *响应时间*，与常规文件系统的评价指标不同，二者的适用场景可能不一致

对于我们的研究内容，市面上也有其他系统是针对这个问题设计的，WAFL 采用树形结构来维护磁盘 update，而 Calavervas 对每个文件事务都单独提交，也使用了与我们类似的 journal 功能

相比之下，我们提出了 batch 的思想，将多个事务延迟到一起提交，虽然牺牲了响应时间，却提高了吞吐量

在 NFS server 上实现了 journal 适配的方法，我们认为有两种：
1. 更频繁地提交事务，缩短周转时间
2. 并且将文件数据的 update 也写入 journal 层，提高磁盘可靠性

并且，我们的 journal 系统可以被多个 fs 共用，这进一步提高了性能

## Conclusions
本文描述的 journal 系统设计，能够有效提高系统恢复时的性能

不过本系统，没有对文件数据提供保护，这就要求尽快将文件写入磁盘，与延迟写入的功能冲突，因此不适用于 */tmp* 文件系统

该系统的实现只需要调用现有的 IO 接口，实现新机制即可，修改量较少

最后，本设计在现有的 ext2fs 的基础上设计，与原有的文件系统完全兼容，因此可在现有代码上直接更新
