# 18.5 Improving IPC by Kernel Design

接下来我们讨论微内核里面一个非常重要的问题：IPC 的速度。首先让我展示一个非常简单，但是也非常慢的设计。这个设计基于 Unix Pipe。我之所以介绍这种方法，是因为一些早期的微内核以一种类似的方式实现的 IPC，而这种方式实际上很慢。

假设我们有两个进程，P1 和 P2，P1 想要给 P2 发送消息。这里该怎么工作呢？一种可能是使用 send 系统调用，传入你想将消息发送到的线程的 ID，以及你想发送消息的指针。这个系统调用会跳到内核中，假设我们是基于 XV6 的 pipe 来实现，那么这里会有一个缓存。或许 P2 正在做一些其他的事情，并没有准备好处理 P1 的消息，所以消息会被先送到内核的缓存中。所以当你调用 send 系统调用，它会将你的消息追加到一个缓存中等待 P2 来接收它。在实际中，几乎很少情况你会只想要发送一个消息，你几乎总是想要能再得到一个回复。所以 P1 在调用完 send 系统调用之后，会立即调用 recv 来获取回复。但是现在让我们先假设我们发送的就是单向的 IPC 消息，send 会将你的消息追加到位于内核的缓存中，我们需要从用户空间将消息逐字节地拷贝到内核的缓存中。之后再返回，这样 P1 可以做一些其他的事情，或许是做好准备去接受回复消息。

![](<../assets/image (152).png>)

过了一会，P2 可以接收消息了，它会调用 recv 系统调用，这个系统调用会返回发送消息线程的 ID，并将消息从内核拷贝到 P2 的内存中。所以这里会从内核缓存中取出最前的消息，并拷贝到 P2 的内存中，之后再返回。

![](<../assets/image (62).png>)

这种方式被称为异步传输，因为 P1 发完消息之后，只是向缓存队列中追加了一条消息，并没有做任何等待就返回了。同时这样的系统这也被称作是 buffered system，因为在发送消息时，内核将每条消息都拷贝到了内部的缓存中，之后当接收消息时，又从 buffer 中将消息拷贝到了目标线程。所以这种方法是异步 buffered。

![](<../assets/image (82).png>)

如果 P1 要完成一次完整的消息发送和接收，那么可以假设有两个 buffer，一个用来发送消息，一个用来接收消息。P1 会先调用 send，send 返回之后。之后 P1 会立即调用 recv，recv 会等待接收消息的 buffer 出现数据，所以 P1 会出让 CPU。在一个单 CPU 的系统中，只有当 P1 出让了 CPU，P2 才可以运行。论文中的讨论是基于单 CPU 系统，所以 P1 先执行，之后 P1 不再执行，出让 CPU 并等待回复消息。这时，P2 才会被调度，之后 P2 调用 recv，拷贝消息。之后 P2 自己再调用 send 将回复消息追加到 buffer，之后 P2 的 send 系统调用返回。假设在某个时间，或许因为定时器中断触发导致 P2 出让 CPU，这时 P1 可以恢复运行，内核发现在接收消息 buffer 有了一条消息，会返回到用户空间的 P1 进程。

![](<../assets/image (68).png>)

这意味着在这个慢的设计中，为了让消息能够发送和回复，将要包含：

- 4 个系统调用，两个 send，两个 recv
- 对应 8 次用户空间内核空间之间的切换，而每一次切换明显都会很慢
- 在 recv 的时候，需要通过 sleep 来等待数据出现
- 并且需要至少一次线程调度和 context switching 来从 P1 切换到 P2

每一次用户空间和内核空间之间的切换和 context switching 都很费时，因为每次切换，都需要切换 Page Table，进而清空 TLB，也就是虚拟内存的查找缓存，这些操作很费时。所以这是一种非常慢的实现方式，它包含了大量的用户空间和内核空间之间的切换、消息的拷贝、缓存的分配等等。实际中，对于这里的场景：发送一个消息并期待收到回复，你可以抛开这种方法并获得简单的多的设计，L4 就是采用了后者。

有关简单的设计在一篇著名的论文中有提到，论文是[Improving IPC by Kernel Design](https://www.cse.unsw.edu.au/~cs9242/19/papers/Liedtke_93.pdf)，这篇论文在今天要讨论的论文前几年发布。相比上面的慢设计，它有几点不同：

- 其中一点是，它是同步的（Synchronized）。所以这里不会丢下消息并等待另一个进程去获取消息，这里的 send 会等待消息被接收，并且 recv 会等待回复消息被发送。如果我是进程 P1，我想要发送消息，我会调用 send。send 并不会拷贝我的消息到内核的缓存中，P1 的 send 会等待 P2 调用 recv。P2 要么已经在内核中等待接收消息，要么 P1 的 send 就要等 P2 下一次调用 recv。当 P1 和 P2 都到达了内核中，也就是 P1 因为调用 send 进入内核，P2 因为调用 recv 进入内核，这时才会发生一些事情。这种方式快的一个原因是，如果 P2 已经在 recv 中，P1 在内核中执行 send 可以直接跳回到 P2 的用户空间，从 P2 的角度来看，就像是从 recv 中返回一样，这样就不需要 context switching 或者线程调度。相比保存寄存器，出让 CPU，通过线程调度找到一个新的进程来运行，这是一种快得多的方式。P1 的 send 知道有一个正在等待的 recv，它会立即跳转到 P2，就像 P2 从自己的 recv 系统调用返回一样。这种方式也被称为 unbuffered。它不需要 buffer 一部分原因是因为它是同步的。

![](<../assets/image (19).png>)

- 当 send 和 recv 都在内核中时，内核可以直接将消息从用户空间 P1 拷贝到用户空间 P2，而不用先拷贝到内核中，再从内核中拷出来。因为现在消息收发的两端都在等待另一端系统调用，这意味着它们消息收发两端的指针都是确定的。recv 会指定它想要消息被投递的位置，所以在这个时间点，我们知道两端的数据内存地址，内核可以直接拷贝消息，而不是需要先拷贝到内核。
- 如果消息超级小，比如说只有几十个字节，它可以在寄存器中传递，而不需要拷贝，你可以称之为 Zero Copy。前面说过，发送方只会在 P2 进入到 recv 时继续执行，之后发送方 P1 会直接跳转到 P2 进程中。从 P1 进入到内核的过程中保存 P1 的用户寄存器，这意味着，如果 P1 要发送的消息很短，它可以将消息存放到特定的寄存器中。当内核返回到 P2 进程的用户空间时，会恢复保存了的寄存器，这意味着当内核从 recv 系统调用返回时，特定寄存器的内容就是消息的内容，因此完全不需要从内存拷贝到内存，也不需要移动数据，消息就存放在寄存器中，可以非常快的访问到。当然，这只对短的消息生效。

![](<../assets/image (9).png>)

- 对于非常长的消息，L4 可以在一个 IPC 消息中携带一个 Page 映射，所以对于巨大的消息，比如说从一个文件读取数据，你可以发送一个物理内存 Page，这个 Page 会被再次映射到目标 Task 地址空间，这里也没有拷贝。这里提供的是共享 Page 的权限。所以短的消息很快，非常长的消息也非常快。对于长的消息，你需要调整目的 Task 的 Page Table，但是这仍然比拷贝快的多。

![](<../assets/image (129).png>)

- 最后一个 L4 使用的技巧是，如果它发现这是个 RPC，有 request 和 response，并且有非常标准的系统调用包括了 send 和 recv，你或许会结合这两个系统调用，以减少用户态和内核态的切换。所以对于 RPC 这种特别的场景，同时也是人们使用 IPC 的一个常见场景，有一个 call 系统调用，它基本上结合了 send 和 recv，区别是这里不会像两个独立的系统调用一样，先返回到用户空间，再次进入到内核空间。在消息的接收端，会有一个 sendrecv 系统调用将回复发出，之后等待来自任何人的 request 消息。这里基本是发送一个回复再加上等待接收下一个 request，这样可以减少一半的内核态和用户态切换。

![](<../assets/image (99).png>)

实际中，所有的这些优化，对于短的 RPC 请求这样一个典型的场景，可以导致 20 倍速度的提升。这是论文中给出的对比之前慢设计提升的性能倍数。这个数字很了不起。Improving IPC by Kernel Design 这篇论文是由今天这篇论文的同一个作者在前几年发表的，因为现在 IPC 可以变得非常的快，它使得人们可以更加认同微内核。

![](<../assets/image (139).png>)

> 学生提问：当使用这些系统调用时，进程是什么时候发送和接收消息的？
>
> Robert 教授：对于包含 request 和 response 的 RPC，进程使用 call 和 sendrecv 这一对系统调用，而不是 send 和 recv。对于 call，你会传入两个参数，你想要发送的消息，以及你要存放回复消息的位置，这个系统调用在内核中会结合发送和接收两个功能。你可以认为这是一种 hack，因为 IPC 使用的是如此频繁，它值得一些 hack 来使得它变得更快。
>
> 学生提问：在上面的图中，P2 会调用 recv 系统调用，P2 怎么知道应该去调用这个系统调用？
>
> Robert 教授：在 RPC 的世界中，我们有 client 会发送 request 到 server，server 会做一些事情并返回。因为 P2 是一个 server，我们会假设 P2 会一直在一个 while 循环中，它随时准备从任何 client 接收消息，做一些数据处理工作，比如在数据库中查找数据，之后再发送回复，然后再回到循环的最开始再等待接收消息。所以我们期望 P2 将所有时间都花费在等待从任何一个客户端接收消息上。前面讨论的设计需要依赖 P2 进程在暂停运行时，一直位于内核的 recv 系统调用中，并等待下一个 request。这样，下一个 request 才可以直接从这个系统调用返回，这种快速路径在这里的设计中超级有效率。
>
> 学生提问：这里提到从 P1 返回到 P2，为了能返回到 P1，需要 P2 发送 response 吗？
>
> Robert 教授：是的，我们期望 P2 发送一个 response，发送 response 与发送 request 是同一个代码路径，只是方向相反（之前是 P1 到 P2 现在是 P2 到 P1），所以当 P2 发送一个 response，这会导致返回到 P1。P1 实际调用的是 call 系统调用，通过从 call 系统调用返回到 P1，会将 P2 的 response 送到 P1。
>
> 这里与你们以为的通常的设置略有不同，通常情况下，你从 P1 通过系统调用进入到内核，在内核中执行系统调用然后再返回，所有的工作都在 P1 这边，这也是 pipe 的 read/write 的工作方式。在这里，P1 进入到内核，但是却返回到了 P2。所以这里有点奇怪，但是却非常的快。
