# 3.6 GFS 写文件（Write File）（1）

GFS 写文件的过程会更加复杂且有趣。从应用程序的角度来看，写文件和读文件的接口是非常类似的，它们都是调用 GFS 的库。写文件是，应用程序会告诉库函数说，我想对这个文件名的文件在这个数据段写入当前存在 buffer 中的数据。让我（Robert 教授）稍微收敛一下，我只想讨论数据的追加。所以我会限制这里的客户端接口，客户端（也就是应用程序）只能说，我想把 buffer 中的数据，追加到这个文件名对应的文件中。这就是 GFS 论文中讨论的记录追加（Record Append）。所以，再次描述一下，对于写文件，客户端会向 Master 节点发送请求说：我想向这个文件名对应的文件追加数据，请告诉我文件中最后一个 Chunk 的位置。

当有多个客户端同时写同一个文件时，一个客户端并不能知道文件究竟有多长。因为如果只有一个客户端在写文件，客户端自己可以记录文件长度，而多个客户端时，一个客户端没法知道其他客户端写了多少。例如，不同客户端写同一份日志文件，没有一个客户端会知道文件究竟有多长，因此也就不知道该往什么样的偏移量，或者说向哪个 Chunk 去追加数据。这个时候，客户端可以向 Master 节点查询哪个 Chunk 服务器保存了文件的最后一个 Chunk。

对于读文件来说，可以从任何最新的 Chunk 副本读取数据，但是对于写文件来说，必须要通过 Chunk 的主副本（Primary Chunk）来写入。对于某个特定的 Chunk 来说，在某一个时间点，Master 不一定指定了 Chunk 的主副本。所以，写文件的时候，需要考虑 Chunk 的主副本不存在的情况。

![](<../assets/image (244).png>)

对于 Master 节点来说，如果发现 Chunk 的主副本不存在，Master 会找出所有存有 Chunk 最新副本的 Chunk 服务器。如果你的一个系统已经运行了很长时间，那么有可能某一个 Chunk 服务器保存的 Chunk 副本是旧的，比如说还是昨天或者上周的。导致这个现象的原因可能是服务器因为宕机而没有收到任何的更新。所以，Master 节点需要能够在 Chunk 的多个副本中识别出，哪些副本是新的，哪些是旧的。所以第一步是，找出新的 Chunk 副本。这一切都是在 Master 节点发生，因为，现在是客户端告诉 Master 节点说要追加某个文件，Master 节点需要告诉客户端向哪个 Chunk 服务器（也就是 Primary Chunk 所在的服务器）去做追加操作。所以，Master 节点的部分工作就是弄清楚在追加文件时，客户端应该与哪个 Chunk 服务器通信。

![](<../assets/image (245).png>)

每个 Chunk 可能同时有多个副本，最新的副本是指，副本中保存的版本号与 Master 中记录的 Chunk 的版本号一致。Chunk 副本中的版本号是由 Master 节点下发的，所以 Master 节点知道，对于一个特定的 Chunk，哪个版本号是最新的。这就是为什么 Chunk 的版本号在 Master 节点上需要保存在磁盘这种非易失的存储中的原因（见 3.4），因为如果版本号在故障重启中丢失，且部分 Chunk 服务器持有旧的 Chunk 副本，这时，Master 是没有办法区分哪个 Chunk 服务器的数据是旧的，哪个 Chunk 服务器的数据是最新的。

> 学生提问：为什么不将所有 Chunk 服务器上保存的最大版本号作为 Chunk 的最新版本号？
>
> Robert 教授：当 Master 重启时，无论如何都需要与所有的 Chunk 服务器进行通信，因为 Master 需要确定哪个 Chunk 服务器存了哪个 Chunk。你可能会想到，Master 可以将所有 Chunk 服务器上的 Chunk 版本号汇总，找出里面的最大值作为最新的版本号。如果所有持有 Chunk 的服务器都响应了，那么这种方法是没有问题的。但是存在一种风险，当 Master 节点重启时，可能部分 Chunk 服务器离线或者失联或者自己也在重启，从而不能响应 Master 节点的请求。所以，Master 节点可能只能获取到持有旧副本的 Chunk 服务器的响应，而持有最新副本的 Chunk 服务器还没有完成重启，或者还是离线状态（这个时候 Master 能找到的 Chunk 最大版本明显不对）。
>
> 当 Master 找不到持有最新 Chunk 的服务器时该怎么办？Master 节点会定期与 Chunk 服务器交互来查询它们持有什么样版本的 Chunk。假设 Master 保存的 Chunk 版本是 17，但是又没有找到存储了版本号是 17 的 Chunk 服务器，那么有两种可能：要么 Master 会等待，并不响应客户端的请求；要么会返回给客户端说，我现在还不知道 Chunk 在哪，过会再重试吧。比如说机房电源故障了，所有的服务器都崩溃了，我们正在缓慢的重启。Master 节点和一些 Chunk 服务器可能可以先启动起来，一些 Chunk 服务器可能要 5 分钟以后才能重启，这种场景下，我们需要等待，甚至可能是永远等待，因为你不会想使用 Chunk 的旧数据。
>
> 所以，总的来说，在重启时，因为 Master 从磁盘存储的数据知道 Chunk 对应的最新版本，Master 节点会整合具有最新版本 Chunk 的服务器。每个 Chunk 服务器会记住本地存储 Chunk 对应的版本号，当 Chunk 服务器向 Master 汇报时，就可以说，我有这个 Chunk 的这个版本。而 Master 节点就可以忽略哪些版本号与已知版本不匹配的 Chunk 服务器。

回到之前的话题，当客户端想要对文件进行追加，但是又不知道文件尾的 Chunk 对应的 Primary 在哪时，Master 会等所有存储了最新 Chunk 版本的服务器集合完成，然后挑选一个作为 Primary，其他的作为 Secondary。

![](<../assets/image (246).png>)

之后，Master 会增加版本号，并将版本号写入磁盘，这样就算故障了也不会丢失这个数据。

![](<../assets/image (247).png>)

接下来，Master 节点会向 Primary 和 Secondary 副本对应的服务器发送消息并告诉它们，谁是 Primary，谁是 Secondary，Chunk 的新版本是什么。Primary 和 Secondary 服务器都会将版本号存储在本地的磁盘中。这样，当它们因为电源故障或者其他原因重启时，它们可以向 Master 报告本地保存的 Chunk 的实际版本号。

> 学生提问：如果 Chunk 服务器上报的版本号高于 Master 存储的版本号会怎么样？
>
> Robert 教授：这个问题很好。我不知道答案，不过论文中有一些线索。其实刚刚我的介绍有一些错误，我认为你的问题让我明白了一些事情。GFS 论文说，如果 Master 节点重启，并且与 Chunk 服务器交互，同时一个 Chunk 服务器重启，并上报了一个比 Master 记住的版本更高的版本。Master 会认为它在分配新的 Primary 服务器时出现了错误，并且会使用这个更高的版本号来作为 Chunk 的最新版本号。
>
> 当 Master 向 Primary 和 Secondary 发送完消息之后就崩溃了，可能会出现上面这种情况。为了让 Master 能够处理这种情况，Master 在发送完消息之后，需要将 Chunk 的最新版本写入到磁盘中。这里的写入或许需要等到 Primary 和 Secondary 返回确认消息之后。

![](<../assets/image (248).png>)

> 学生提问：听不清（但是应该与这一节的第一个问题一样）。
>
> Robert 教授：我不认为这行得通。因为存在这种可能性，当 Master 节点重启时，存储了 Chunk 最新版本号的 Chunk 服务器是离线状态。这种情况下，我们不希望 Master 在重启时不知道当前的版本号，因为那样的话，Master 就会认为当前发现的最高版本号是当前版本号，但是（由于有最新版本号的 Chunk 服务器还是离线状态）发现的最高版本号可能是个旧版本号。
>
> 我（Robert 教授）之前没太关注这块，所以我也不太确定 Master 究竟是先写本地磁盘中的版本号，然后再通知 Primary 和 Secondary，还是反过来。但是不管怎么样，Master 会更新自己的版本号，并通知 Primary 和 Secondary 说，你们现在是 Primary 和 Secondary，并且版本号更新了。

所以，现在我们有了一个 Primary，它可以接收来自客户端的写请求，并将写请求应用在多个 Chunk 服务器中。之所以要管理 Chunk 的版本号，是因为这样 Master 可以将实际更新 Chunk 的能力转移给 Primary 服务器。并且在将版本号更新到 Primary 和 Secondary 服务器之后，如果 Master 节点故障重启，还是可以在相同的 Primary 和 Secondary 服务器上继续更新 Chunk。

现在，Master 节点通知 Primary 和 Secondary 服务器，你们可以修改这个 Chunk。它还给 Primary 一个租约，这个租约告诉 Primary 说，在接下来的 60 秒中，你将是 Primary，60 秒之后你必须停止成为 Primary。这种机制可以确保我们不会同时有两个 Primary，我们之后会再做讨论（3.7 的问答中有一个专门的问题讨论）。

![](<../assets/image (249).png>)

我们现在来看 GFS 论文的图 2。假设现在 Master 节点告诉客户端谁是 Primary，谁是 Secondary，GFS 提出了一种聪明的方法来实现写请求的执行序列。客户端会将要追加的数据发送给 Primary 和 Secondary 服务器，这些服务器会将数据写入到一个临时位置。所以最开始，这些数据不会追加到文件中。当所有的服务器都返回确认消息说，已经有了要追加的数据，客户端会向 Primary 服务器发送一条消息说，你和所有的 Secondary 服务器都有了要追加的数据，现在我想将这个数据追加到这个文件中。Primary 服务器或许会从大量客户端收到大量的并发请求，Primary 服务器会以某种顺序，一次只执行一个请求。对于每个客户端的追加数据请求（也就是写请求），Primary 会查看当前文件结尾的 Chunk，并确保 Chunk 中有足够的剩余空间，然后将客户端要追加的数据写入 Chunk 的末尾。并且，Primary 会通知所有的 Secondary 服务器也将客户端要追加的数据写入在它们自己存储的 Chunk 末尾。这样，包括 Primary 在内的所有副本，都会收到通知将数据追加在 Chunk 的末尾。

![](<../assets/image (250).png>)

但是对于 Secondary 服务器来说，它们可能可以执行成功，也可能会执行失败，比如说磁盘空间不足，比如说故障了，比如说 Primary 发出的消息网络丢包了。如果 Secondary 实际真的将数据写入到了本地磁盘存储的 Chunk 中，它会回复“yes”给 Primary。如果所有的 Secondary 服务器都成功将数据写入，并将“yes”回复给了 Primary，并且 Primary 也收到了这些回复。Primary 会向客户端返回写入成功。如果至少一个 Secondary 服务器没有回复 Primary，或者回复了，但是内容却是：抱歉，一些不好的事情发生了，比如说磁盘空间不够，或者磁盘故障了，Primary 会向客户端返回写入失败。

![](<../assets/image (251).png>)

GFS 论文说，如果客户端从 Primary 得到写入失败，那么客户端应该重新发起整个追加过程。客户端首先会重新与 Master 交互，找到文件末尾的 Chunk；之后，客户端需要重新发起对于 Primary 和 Secondary 的数据追加操作。
