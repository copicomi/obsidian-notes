# 3.4 GFS Master 节点

接下来看看 GFS 的大致架构，这在论文的图 1 中也有介绍。

假设我们有上百个客户端和一个 Master 节点。尽管实际中可以拿多台机器作为 Master 节点，但是 GFS 中 Master 是 Active-Standby 模式，所以只有一个 Master 节点在工作。Master 节点保存了文件名和存储位置的对应关系。除此之外，还有大量的 Chunk 服务器，可能会有数百个，每一个 Chunk 服务器上都有 1-2 块磁盘。

![](<../assets/image (237).png>)

在这里，Master 节点用来管理文件和 Chunk 的信息，而 Chunk 服务器用来存储实际的数据。这是 GFS 设计中比较好的一面，它将这两类数据的管理问题几乎完全隔离开了，这样这两个问题可以使用独立设计来解决。Master 节点知道每一个文件对应的所有的 Chunk 的 ID，这些 Chunk 每个是 64MB 大小，它们共同构成了一个文件。如果我有一个 1GB 的文件，那么 Master 节点就知道文件的第一个 Chunk 存储在哪，第二个 Chunk 存储在哪，等等。当我想读取这个文件中的任意一个部分时，我需要向 Master 节点查询对应的 Chunk 在哪个服务器上，之后我可以直接从 Chunk 服务器读取对应的 Chunk 数据。

更进一步，我们看一下 GFS 的一致性以及 GFS 是如何处理故障。为了了解这些，我们需要知道 Master 节点内保存的数据内容，这里我们关心的主要是两个表单：

- 第一个是文件名到 Chunk ID 或者 Chunk Handle 数组的对应。这个表单告诉你，文件对应了哪些 Chunk。但是只有 Chunk ID 是做不了太多事情的，所以有了第二个表单。
- 第二个表单记录了 Chunk ID 到 Chunk 数据的对应关系。这里的数据又包括了：
  - 每个 Chunk 存储在哪些服务器上，所以这部分是 Chunk 服务器的列表
  - 每个 Chunk 当前的版本号，所以 Master 节点必须记住每个 Chunk 对应的版本号。
  - 所有对于 Chunk 的写操作都必须在主 Chunk（Primary Chunk）上顺序处理，主 Chunk 是 Chunk 的多个副本之一。所以，Master 节点必须记住哪个 Chunk 服务器持有主 Chunk。
  - 并且，主 Chunk 只能在特定的租约时间内担任主 Chunk，所以，Master 节点要记住主 Chunk 的租约过期时间。

![](<../assets/image (238).png>)

以上数据都存储在内存中，如果 Master 故障了，这些数据就都丢失了。为了能让 Master 重启而不丢失数据，Master 节点会同时将数据存储在磁盘上。所以 Master 节点读数据只会从内存读，但是写数据的时候，至少有一部分数据会接入到磁盘中。更具体来说，Master 会在磁盘上存储 log，每次有数据变更时，Master 会在磁盘的 log 中追加一条记录，并生成 CheckPoint（类似于备份点）。

![](<../assets/image (239).png>)

有些数据需要存在磁盘上，而有些不用。它们分别是：

- Chunk Handle 的数组（第一个表单）要保存在磁盘上。我给它标记成 NV（non-volatile, 非易失），这个标记表示对应的数据会写入到磁盘上。
- Chunk 服务器列表不用保存到磁盘上。因为 Master 节点重启之后可以与所有的 Chunk 服务器通信，并查询每个 Chunk 服务器存储了哪些 Chunk，所以我认为它不用写入磁盘。所以这里标记成 V（volatile），
- 版本号要不要写入磁盘取决于 GFS 是如何工作的，我认为它需要写入磁盘。我们之后在讨论系统是如何工作的时候再详细讨论这个问题。这里先标记成 NV。
- 主 Chunk 的 ID，几乎可以确定不用写入磁盘，因为 Master 节点重启之后会忘记谁是主 Chunk，它只需要等待 60 秒租约到期，那么它知道对于这个 Chunk 来说没有主 Chunk，这个时候，Master 节点可以安全指定一个新的主 Chunk。所以这里标记成 V。
- 类似的，租约过期时间也不用写入磁盘，所以这里标记成 V。

![](<../assets/image (240).png>)

任何时候，如果文件扩展到达了一个新的 64MB，需要新增一个 Chunk 或者由于指定了新的主 Chunk 而导致版本号更新了，Master 节点需要向磁盘中的 Log 追加一条记录说，我刚刚向这个文件添加了一个新的 Chunk 或者我刚刚修改了 Chunk 的版本号。所以每次有这样的更新，都需要写磁盘。GFS 论文并没有讨论这么多细节，但是因为写磁盘的速度是有限的，写磁盘会导致 Master 节点的更新速度也是有限的，所以要尽可能少的写入数据到磁盘。

这里在磁盘中维护 log 而不是数据库的原因是，数据库本质上来说是某种 B 树（b-tree）或者 hash table，相比之下，追加 log 会非常的高效，因为你可以将最近的多个 log 记录一次性的写入磁盘。因为这些数据都是向同一个地址追加，这样只需要等待磁盘的磁碟旋转一次。而对于 B 树来说，每一份数据都需要在磁盘中随机找个位置写入。所以使用 Log 可以使得磁盘写入更快一些。

当 Master 节点故障重启，并重建它的状态，你不会想要从 log 的最开始重建状态，因为 log 的最开始可能是几年之前，所以 Master 节点会在磁盘中创建一些 checkpoint 点，这可能要花费几秒甚至一分钟。这样 Master 节点重启时，会从 log 中的最近一个 checkpoint 开始恢复，再逐条执行从 Checkpoint 开始的 log，最后恢复自己的状态。
