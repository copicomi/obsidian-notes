# 3.8 GFS 的一致性

或许这里最重要的部分就是重复我们刚刚（3.7 的问答中）讨论过的内容。

当我们追加数据时，面对 Chunk 的三个副本，当客户端发送了一个追加数据的请求，要将数据 A 追加到文件末尾，所有的三个副本，包括一个 Primary 和两个 Secondary，都成功的将数据追加到了 Chunk，所以 Chunk 中的第一个记录是 A。

![](<../assets/image (252).png>)

假设第二个客户端加入进来，想要追加数据 B，但是由于网络问题发送给某个副本的消息丢失了。所以，追加数据 B 的消息只被两个副本收到，一个是 Primary，一个是 Secondary。这两个副本都在文件中追加了数据 B，所以，现在我们有两个副本有数据 B，另一个没有。

![](<../assets/image (253).png>)

之后，第三个客户端想要追加数据 C，并且第三个客户端记得下图中左边第一个副本是 Primary。Primary 选择了偏移量，并将偏移量告诉 Secondary，将数据 C 写在 Chunk 的这个位置。三个副本都将数据 C 写在这个位置。

![](<../assets/image (254).png>)

对于数据 B 来说，客户端会收到写入失败的回复，客户端会重发写入数据 B 的请求。所以，第二个客户端会再次请求追加数据 B，或许这次数据没有在网络中丢包，并且所有的三个副本都成功追加了数据 B。现在三个副本都在线，并且都有最新的版本号。

![](<../assets/image (255).png>)

之后，如果一个客户端读文件，读到的内容取决于读取的是 Chunk 的哪个副本。客户端总共可以看到三条数据，但是取决于不同的副本，读取数据的顺序是不一样的。如果读取的是第一个副本，那么客户端可以读到 A、B、C，然后是一个重复的 B。如果读取的是第三个副本，那么客户端可以读到 A，一个空白数据，然后是 C、B。所以，如果读取前两个副本，B 和 C 的顺序是先 B 后 C，如果读的是第三个副本，B 和 C 的顺序是先 C 后 B。所以，不同的读请求可能得到不同的结果。

或许最坏的情况是，一些客户端写文件时，因为其中一个 Secondary 未能成功执行数据追加操作，客户端从 Primary 收到写入失败的回复。在客户端重新发送写文件请求之前，客户端就故障了。所以，你有可能进入这种情形：数据 D 出现在某些副本中，而其他副本则完全没有。

![](<../assets/image (256).png>)

在 GFS 的这种工作方式下，如果 Primary 返回写入成功，那么一切都还好，如果 Primary 返回写入失败，就不是那么好了。Primary 返回写入失败会导致不同的副本有完全不同的数据。

> 学生提问：客户端重新发起写入的请求时从哪一步开始重新执行的？
>
> Robert 教授：根据我从论文中读到的内容，（当写入失败，客户端重新发起写入数据请求时）客户端会从整个流程的最开始重发。客户端会再次向 Master 询问文件最后一个 Chunk 是什么，因为文件可能因为其他客户端的数据追加而发生了改变。
>
> 学生提问：为什么 GFS 要设计成多个副本不一致？
>
> Robert 教授：我不明白 GFS 设计者为什么要这么做。GFS 可以设计成多个副本是完全精确同步的，你们在 lab2 和 lab3 会设计一个系统，其中的副本是同步的。并且你们也会知道，为了保持同步，你们要使用各种各样的技术。如果你们想要让副本保持同步，其中一条规则就是你们不能允许这种只更新部分服务器的不完整操作。这意味着，你必须要有某种机制，即使客户端挂了，系统仍然会完成请求。如果这样的话，GFS 中的 Primary 就必须确保每一个副本都得到每一条消息。
>
> 学生提问：如果第一次写 B 失败了，C 应该在 B 的位置吧？
>
> Robert 教授：实际上并没有。实际上，Primary 将 C 添加到了 Chunk 的末尾，在 B 第一次写入的位置之后。我认为这么做的原因是，当写 C 的请求发送过来时，Primary 实际上可能不知道 B 的命运是什么。因为我们面对的是多个客户端并发提交追加数据的请求，为了获得高性能，你会希望 Primary 先执行追加数据 B 的请求，一旦获取了下一个偏移量，再通知所有的副本执行追加数据 C 的请求，这样所有的事情就可以并行的发生。
>
> 也可以减慢速度，Primary 也可以判断 B 已经写入失败了，然后再发一轮消息让所有副本撤销数据 B 的写操作，但是这样更复杂也更慢。

GFS 这样设计的理由是足够的简单，但是同时也给应用程序暴露了一些奇怪的数据。这里希望为应用程序提供一个相对简单的写入接口，但应用程序需要容忍读取数据的乱序。如果应用程序不能容忍乱序，应用程序要么可以通过在文件中写入序列号，这样读取的时候能自己识别顺序，要么如果应用程序对顺序真的非常敏感那么对于特定的文件不要并发写入。例如，对于电影文件，你不会想要将数据弄乱，当你将电影写入文件时，你可以只用一个客户端连续顺序而不是并发的将数据追加到文件中。

有人会问，如何将这里的设计转变成强一致的系统，从而与我们前面介绍的单服务器模型更接近，也不会产生一些给人“惊喜”的结果。实际上我不知道怎么做，因为这需要完全全新的设计。目前还不清楚如何将 GFS 转变成强一致的设计。但是，如果你想要将 GFS 升级成强一致系统，我可以为你列举一些你需要考虑的事情：

- 你可能需要让 Primary 来探测重复的请求，这样第二个写入数据 B 的请求到达时，Primary 就知道，我们之前看到过这个请求，可能执行了也可能没执行成功。Primay 要尝试确保 B 不会在文件中出现两次。所以首先需要的是探测重复的能力。
- 对于 Secondary 来说，如果 Primay 要求 Secondary 执行一个操作，Secondary 必须要执行而不是只返回一个错误给 Primary。对于一个严格一致的系统来说，是不允许 Secondary 忽略 Primary 的请求而没有任何补偿措施的。所以我认为，Secondary 需要接受请求并执行它们。如果 Secondary 有一些永久性故障，例如磁盘被错误的拔出了，你需要有一种机制将 Secondary 从系统中移除，这样 Primary 可以与剩下的 Secondary 继续工作。但是 GFS 没有做到这一点，或者说至少没有做对。
- 当 Primary 要求 Secondary 追加数据时，直到 Primary 确信所有的 Secondary 都能执行数据追加之前，Secondary 必须小心不要将数据暴露给读请求。所以对于写请求，你或许需要多个阶段。在第一个阶段，Primary 向 Secondary 发请求，要求其执行某个操作，并等待 Secondary 回复说能否完成该操作，这时 Secondary 并不实际执行操作。在第二个阶段，如果所有 Secondary 都回复说可以执行该操作，这时 Primary 才会说，好的，所有 Secondary 执行刚刚你们回复可以执行的那个操作。这是现实世界中很多强一致系统的工作方式，这被称为两阶段提交（Two-phase commit）。
- 另一个问题是，当 Primary 崩溃时，可能有一组操作由 Primary 发送给 Secondary，Primary 在确认所有的 Secondary 收到了请求之前就崩溃了。当一个 Primary 崩溃了，一个 Secondary 会接任成为新的 Primary，但是这时，新 Primary 和剩下的 Secondary 会在最后几个操作有分歧，因为部分副本并没有收到前一个 Primary 崩溃前发出的请求。所以，新的 Primary 上任时，需要显式的与 Secondary 进行同步，以确保操作历史的结尾是相同的。
- 最后，时不时的，Secondary 之间可能会有差异，或者客户端从 Master 节点获取的是稍微过时的 Secondary。系统要么需要将所有的读请求都发送给 Primary，因为只有 Primary 知道哪些操作实际发生了，要么对于 Secondary 需要一个租约系统，就像 Primary 一样，这样就知道 Secondary 在哪些时间可以合法的响应客户端。

为了实现强一致，以上就是我认为的需要在系统中修复的东西，它们增加了系统的复杂度，增加了系统内部组件的交互。我也是通过思考课程的实验，得到上面的列表的，你们会在 lab2，3 中建立一个强一致系统，并完成所有我刚刚说所有的东西。

最后，让我花一分钟来介绍 GFS 在它生涯的前 5-10 年在 Google 的出色表现，总的来说，它取得了巨大的成功，许多许多 Google 的应用都使用了它，许多 Google 的基础架构，例如 BigTable 和 MapReduce 是构建在 GFS 之上，所以 GFS 在 Google 内部广泛被应用。它最严重的局限可能在于，它只有一个 Master 节点，会带来以下问题：

- Master 节点必须为每个文件，每个 Chunk 维护表单，随着 GFS 的应用越来越多，这意味着涉及的文件也越来越多，最终 Master 会耗尽内存来存储文件表单。你可以增加内存，但是单台计算机的内存也是有上限的。所以，这是人们遇到的最早的问题。
- 除此之外，单个 Master 节点要承载数千个客户端的请求，而 Master 节点的 CPU 每秒只能处理数百个请求，尤其 Master 还需要将部分数据写入磁盘，很快，客户端数量超过了单个 Master 的能力。
- 另一个问题是，应用程序发现很难处理 GFS 奇怪的语义（本节最开始介绍的 GFS 的副本数据的同步，或者可以说不同步）。
- 最后一个问题是，从我们读到的 GFS 论文中，Master 节点的故障切换不是自动的。GFS 需要人工干预来处理已经永久故障的 Master 节点，并更换新的服务器，这可能需要几十分钟甚至更长的而时间来处理。对于某些应用程序来说，这个时间太长了。

（所以我们才需要多副本，多活，高可用，故障自修复的分布式系统啊）
