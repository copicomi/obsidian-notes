[[数据密集型应用系统设计 (Martin Kleppmann) (Z-Library).pdf]]
# P1 数据系统基础
## C1 数据系统的需求
我们将消息队列、数据库、高速缓存这些模块统统视为*数据系统*，如今的应用系统往往是由不同组件架构而成的，这些组件具有复合型，因此不做区分

缓存用redis，异步通信用kafka，数据库用rocksdb，这些都视作数据系统

要研究数据系统的设计，我们需要先搞懂，设计的**需求**是什么：
1. 可靠性
2. 可维护性
3. 可扩展性
### 可靠，如何处理异常情况
分三种异常：
1. 硬件
2. 软件
3. 人为配置

软硬件层面的故障，采用冗余**备份**与充分**测试**就足够了，而为了减少开发者犯错的机会，需要精心设计**接口**与架构，并**解耦**模块
### 可伸缩性，如何应对负载增长
要讨论这个问题，首先要明确，什么是负载？
你需要用参数来描述生产情景，再决定方案
> 这里举了 twitter 关注者与推文的例子
> 
> 当用户查看关注时间线时，有两种方法：
> 1. 获取所有post，再merge
> 2. 维护缓存，由poster将post推入缓存
> 
> 第一种的读入负担很大，而第二种对于超级巨星表现不佳，最后他们使用二者结合的方案
> 
> 粉丝数和关注数就是**负载参数**

在此之后，描述**性能**，如吞吐量与响应时间

在做好充足假设的工作后，我们开始设计架构
### 可维护性，如何投入长期生产
三个方面：
1. 可用性，运维操作必须简单
2. 简单性，高质量的抽象
3. 拓展性，便于系统演化
## C2 数据模型与查询语言
数据模型是最基本的元件，它决定你如何看待世界与问题，并针对现实进行特化，层层封装

### 常见模型
大约以下几种：
1. 关系
2. 文档
3. 网络
4. 层次

层次模型的缺点很明显，难以处理多对多关系与一致性问题

对此提出两种方案，关系型与网络型
二者提出需要抽象出数据间的**联系**，通过这种联系访问数据

但网络模型以图为模型，查询复杂度极高，因此关系模型流行起来

不过关系模型与面向对象之间，具备天然的不匹配，table需要一个中间层转换才能正常访问，结合新的表达力需求，兴起了Nosql的想法

与此同时，文档模型也出现了，采用文本串表达层次数据，又用id区分对象，内容也相对灵活

文档与关系模型，二者的表达力存在互补，因此混合二者的做法也很有潜力

> 我认为json的优点在于插入，无需检查模式，但存储对象仅限于字符串，过于简单，table的优点在于查询，具备充足的层次抽象，但有时固定的模式反而过于死板，造成空间浪费
> 
> 这里就是混合的切入点了，在json中插入文本化的模拟table，在table中添加模拟json对象的属性，便可解决彼此的痛点

### 查询语言
两种：
1. 声明式
2. 命令式

二者的表达力与精确性不同，声明式为优化器提供更大的自由

### 图数据模型
我认为，图模型是更加自由的关系模型，图本质上就是离散数学中的“关系”

常规的关系模型对**联系**的定义更加死板，join属性是静态的，但是如果联系是更为广泛的呢？如果任何数据之间都可能联系呢？

那么此时静态的关系模型就难以表达这种数据池了，才引出表达力更强的图模型，任何数据只要通过edge就能进行访问，而不考虑类型（从这一点来说又具备文档模型的特点）

与网络模型的区别：后者不过是没有摆脱静态类型的镣铐

## C3 存储与检索
决定数据的底层模型后，我们开始考虑操作，如何读写数据呢？
### 基本想法
以 KV 存储为例

模式一：遍历写入，遍历读出，读写都很慢
模式二：追加写入，遍历读出，读出慢
模式三：哈希索引，平衡读写时间

删除就追加写入一个墓碑
### 日志合并
采用追加写入，利用了顺序读写更快的特点，也产生了空间浪费，因此需要合并空间

采用归并排序即可，排序后还可以使用稀疏索引

利用内存数据结构维护有序的SSTables，再构建磁盘上的LSM树
查询时先查内存，再查磁盘

然而有一个问题是，内存中的数据是易失的，因此在写入内存前，需要在磁盘中先写入日志（WAL）

优化方法：
1. 布隆过滤器
2. 调整合并压缩策略

LSMTree的核心在于追加写入带来的空间局部性

### B树
在磁盘上构建索引，由于内部结构是覆写式的，因此也需要[[WAL]]，并发需要使用latch保护数据

优化：
1. 用copy-on-write替代WAL，实现版本回退
2. internalPage采用模糊key
3. leafPage添加左右指针，实现迭代器遍历
4. 优化page布局，提高空间局部性

### 比较B树与LSM树
b树是稳定的数据结构，代价可估计，但随机写入造成瓶颈，读出快
而LSM树为顺序写入，写入快，读出慢，同时后台的合并压缩操作也会影响前台的吞吐量

需要根据具体的负载情况选择设计

### 其他索引
聚集、辅助、多列、多维、复合索引，这些在数据库课程中已经学过

模糊索引不是讨论的重点

以往的数据库以磁盘为核心，这是建立在高成本内存的假设之上的

现在这个假设不再牢固，因此出现像redis的内存数据库

对于小规模的数据集可以完全在缓存中管理，同时不需要考虑磁盘的编码与交互，从而提升性能

而反缓存方法，是将内存装不下的数据暂存到磁盘，使用时再读回，与传统的缓存方向相反，这样使得内存数据库可以支持超量的数据

### 回到现实
读写与索引，我们已经讨论了底层的设计，那么现实中将遇到什么样的系统需求？

我认为两种：
1. 操作密集型（OLTP）
2. 计算密集型（OLAP）

针对二者，甚至针对单个业务情景的优化都是有必要的

分析型业务的建模呈雪花状

常规关系数据库都是按行存储，不过我们也可以改为按列存储，来优化投影操作时的读写次数

列存储后，通过对key排序可以进行压缩，也能利用L1缓存与SIMD指令提升性能，不过写入时就麻烦了，这里可以用到LSM追加写入的特性

以上讨论的是数据组织顺序，我们当然可以针对同一份数据，备份不同的数据库副本，采用不同的组织方式，根据具体业务选择对象

聚合操作通过提前物化计算结果，加速查询操作，但不能体现原数据

### 总结
到这里介绍了两种存储引擎的思想：覆写与追加，并介绍了现实需求是如何影响底层设计的

## C4 编码与演化（TBD）
随版本更迭，数据的内部细节也许会改变，这就引出了兼容性设计的问题
### 编码
通过网络发送或者进行持久化时，需要将内存中的数据结构编码为字节流（Encode），常用格式为json、xml与csv
# P2 分布式数据
有Raft基础，这p略读
[[【实验笔记】Lab3 Raft]]
[[【论文】Raft]]
## C5 复制

